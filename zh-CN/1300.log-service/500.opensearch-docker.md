# OpenSearch 相关说明

## 背景

为了支持 OceanBase V4.x 版本的全链路追踪特性，OCP V3.3.0 新增了数据中台功能，其中包含了全链路追踪和日志检索等功能，这些功能依赖于 OpenSearch 集群来进行存储和查询数据。而且对于私有云输出场景，随 OCP 集群部署 OpenSearch集群也更加方便，能够有效降低支持人员的部署学习成本。

## 前提条件

已参考 **集群规格评估** 对 OpenSearch 集群进行资源规划。

## 部署 OpenSearch 集群

1. 首先要在 OCP 宿主机上获得 OpenSearch 的 Docker 镜像。您可根据实际情况执行以下命令获得所需镜像，下载请参见 [OpenSearch docker 镜像](https://docker.alibaba-inc.com/#/dockerImage/3006225/detail)。

   <main id="notice" type='explain'>
    <h4>说明</h4>
    <p>当前镜像最新版本为 V3.3.2。</p>
   </main>

    ```shell
    # X86 环境
    docker pull reg.docker.alibaba-inc.com/oceanbase-platform/ocp-opensearch:[版本号]-x64
    # arm 环境
    docker pull reg.docker.alibaba-inc.com/oceanbase-platform/ocp-opensearch:[版本号]-arm64
    ```

2. 通过 Docker 部署 OpenSearch。

    1. 生成根证书

        ```shell
        cd ${pem-path}
        openssl genrsa -out ./root-ca-key.pem 2048
        openssl req -new -x509 -sha256 -key ./root-ca-key.pem -subj "/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=ROOT" -out ./root-ca.pem
        ```

    2. 启动容器。

        ```shell
        docker run -d --net host -v ${pem-path}/root-ca-key.pem:/opt/opensearch/config/root-ca-key.pem -v ${pem-path}/root-ca.pem:/opt/opensearch/config/root-ca.pem --env  OPENSEARCH_USERNAME=xxx --env OPENSEARCH_PASSWORD=xxx --env OPENSEARCH_NODE_URLS=ip1,ip2,ip3 --env HOST_IP=127.0.0.1 --env OPENSEARCH_JVM_HEAP=4g --name ocp-opensearch reg.docker.alibaba-inc.com/oceanbase-platform/ocp-opensearch:3.3.2-x64
        ```

        <main id="notice" type='notice'>
          <h4>注意</h4>
          <ul>
          <li>OpenSearch 的 Docker 镜像必须以 host 网络模式启动。</li>
          <li>容器启动命令中的 ${pem-path} 代指存放根证书的目录。</li>
          <li>如果要将宿主机目录挂载到 OpenSearch 目录下，需先将宿主机的挂载目录权限设置为 admin。容器内 admin 用户的 uid 和 gid 均为 500。如果与宿主机的 admin 用户 ID 不一致会导致挂载失败，您可在容器启动后，进入容器内执行 <code>chown admin:admin /data/1/opensearch</code> 后重启容器进行解决。</li>
          <li>多节点部署需要将根证书拷贝到各节点上，再进行启动操作。</li>
          </ul>
        </main>

    启动镜像时，相关环境变量说明如下：

    | 环境变量 | 是否必填 | 说明 |
    |---------|--------|-------|
    | OPENSEARCH_USERNAME | 必填 | OpenSearch 的用户名，同时也是 cerebro 的登录用户名。 |
    | OPENSEARCH_PASSWORD | 必填 | OpenSearch 的密码，同时也是 cerebro 的登录密码。 |
    | OPENSEARCH_NODE_URLS | 必填 | OpenSearch 的节点 list。 |
    | HOST_IP | 选填 | 主机 IP，如果不指定，将通过 `hostname -i` 获取，需要保证 `/etc/hosts` 中正确配置。 |
    | OPENSEARCH_JVM_HEAP | 选填 | OpenSearch 的 jvm 内存，建议在设置为节点内存的一半（最大 32GB），默认 4GB。 |
    | OPENSEARCH_HTTP_PORT | 选填 | OpenSearch 与外界通信的端口，默认是 9200。 |
    | OPENSEARCH_TCP_PORT | 选填 | OpenSearch 节点间通信的端口，默认是 9300。 |
    | CEREBRO_PORT | 选填 | cerebro 端口，默认是 9100。 |
    | ELASTICSEARCH_EXPORTER_PORT | 选填 | elasticsearch_exporter 端口，默认是 9114。 |

3. OCP 连接 OpenSearch。

    在 OCP 启动后，需要在 **系统参数** 中手动添加 OpenSearch 的地址和账密。详情可参考 [配置链路查询相关参数](../1300.log-service/300.configuration-trace-paremeters.md) 。

    |参数名|说明|
    |----|----
    |ocp.analyze.es.client.addresses| 搭建的 opensearch 集群地址。如有多个入口（client）地址，需以逗号分隔。   |
    |ocp.analyze.es.client.username|  搭建的 opensearch 集群用户名。   |
    |ocp.analyze.es.client.password| 搭建的 opensearch 集群密码。     |

## OpenSearch 服务说明

OCP-OpenSearch 的 docker 镜像中集成了三个服务，分别是 opensearch、elasticsearch_exporter 和 cerebro。

### OpenSearch

OpenSearch 是一个社区驱动的基于 Apache Lucene 的分布式搜索和分析引擎，源自 ES 7.10.2，协议是 Apache License 2.0。
将数据添加到 OpenSearch 后，可以对其执行全文搜索：按字段搜索、搜索多个索引、提升字段、按分数对结果进行排名、按字段对结果进行排序以及聚合结果。

### elasticsearch_exporte

elasticsearch_exporter 是一个将 OpenSearch 的各种指标用 Prometheus 格式导出的导出器，已经将其集成到了 ocp_prometheus 中。开源协议是 Apache License 2.0。
通过 elasticsearch_exporter 可以获取 OpenSearch 的监控数据，其中包括索引性能、JVM 内存占用和垃圾回收、集群健康和节点可用性、系统和网络指标等监控数据。

### cerebro

cerebro 是 OpenSearch 管理工具，可以通过 Web 页面对 OpenSearch 进行运维操作，支持索引的创建和删除、修改集群配置参数等。该软件同样开源，协议是 MIT License 2.0。

### 日志位置

日志目录：`/home/admin/logs`。

## OpenSearch 集群规格预估

### 磁盘容量

磁盘容量的影响因素包括如下方面：

* 副本数量。
* 索引的开销：通常比源数据大 10%。
* 操作系统预留：默认保留 5% 的空间。
* OpenSearch 内部开销：段合并、日志等内部操作，一般预留 20%。
* 安全阈值：一般至少预留 20%。

根据以上因素得出：最小磁盘大小 = 源数据量 \*（1 + 副本数）* 1.7，您可根据该公式对磁盘容量进行把控。

### CPU 和内存

OpenSearch 的计算资源主要消耗在写入和查询中，对 CPU 要求不高。内存建议设为 16 GB 、32 GB 或者 64 GB，同时分配给 OpenSearch 的内存不要超过系统可用内存的一半，且不要超过 32 GB。

### Shard 评估

Shard 的大小和数量是影响 OpenSearch 集群稳定性和性能的重要因素之一。

分配建议如下：

* 在分配 shard 前，对 OpenSearch 进行数据测试。当数据量很大时，要减少写入量的大小，降低 OpenSearch 压力，建议选择多主 1 副本；当数据量较小，且写入量也比较小时，可以使用单主多副本或者单主 1 副本。
* 建议一个 shard 的存储量保持在 30GB 以内（最优），特殊情况下可以提升到 50GB 以内。不宜过大或过小，过大会使 ES 的故障恢复速度变慢，过小会导致非常多的分片，而每个分片都会使用一定数量的 CPU 和内存，从而导致读写性能差、内存不足等问题。如果评估结果超过该容量，建议在创建索引之前，合理进行 shard 分配，后期进行 reindex，虽然能保证不停机，但是比较耗时。
* 建议 shard 的个数（包括副本）要尽可能等于数据节点数，或者是数据节点数的整数倍，使得分片在所有节点内均匀分布，更加利于负载均衡。

### OB OpenSearch 集群估算

一个 observer 一天的日志量大概是 6~8 GB，trace 功能还在开发，暂且也算做 8 Gb，默认数据在OpenSearch 中保存一周。因此，一台 observer，副本数量为 1，数据保存 7 天的情况下，所需的OpenSearch 磁盘容量为 380 GB。因此，集群部署规格可以如下表所示：

| observer 数量 | os 集群规模 | 节点规格 | 磁盘容量 |
|---------------|------------|---------|----------|
| 20 | 4 | 8 核 32 GB | 2 TB |
| 50 | 10 | 8 核 32 GB | 2 TB |
| 200 | 20 | 16 核 64 GB | 4 TB |
| 400 | 40 | 16 核 64 GB | 4 TB |